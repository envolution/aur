pkgbase = python-vllm-cuda
	pkgdesc = high-throughput and memory-efficient inference and serving engine for LLMs
	pkgver = 0.7.1
	pkgrel = 2
	url = https://github.com/vllm-project/vllm
	arch = x86_64
	license = Apache-2.0
	makedepends = git
	makedepends = gcc13
	makedepends = cuda
	makedepends = cuda-tools
	depends = python-installer
	depends = python
	depends = python-pytorch
	provides = python-vllm
	conflicts = python-vllm
	source = git+https://github.com/vllm-project/vllm.git#tag=v0.7.1
	sha256sums = b6c46935fd4c89dcb4c8fb056492bfeddc7c0bd3338c3207db25872ba45e309a

pkgname = python-vllm-cuda
